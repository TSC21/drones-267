\documentclass[10pt]{scrartcl} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage[cm]{fullpage}
\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{biblatex} % use biber command to regenerate references

\renewcommand{\bibfont}{\footnotesize}
%\pagenumbering{gobble}
\usepackage{hyperref}
\addbibresource{report.bib}

\title{Accurate Vision-Based Landing For Multicopter UAVs}
\subtitle{CS287: Project Report}
\author{Constantin Berzan, Sunil Shah}
\date{} 

\begin{document}
\maketitle

\begin{abstract}
Multicopter UAVs have become increasingly popular over the last five years and are being considered for commercial purposes. However, they are hampered by their limited battery life. This class project is part of a larger project to build an automatic charging station for multicopters. Currently openly available automated landing algorithms rely on GPS to localise the UAV with respect to the landing station. We show that this approach is considerably inaccurate and implement a vision-based landing system which uses pose estimates to more accurately localise the UAV. 
In order to land based on this data, we build a state machine and controller that interacts with the open source ArduCopter autopilot to bring a test quadcopter down to a landing pattern. While environmental conditions and hardware issues ultimately prevented us from realising a vision-based landing, we are confident that, in good conditions and with some slight modification, our system should be able to land with much greater accuracy than GPS.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
%% SUNIL
%% What is the problem? Why does it matter?
In the recent past, multicopter UAVs (or drones, as they are colloquially known) have become increasingly popular. This has been well documented by both mainstream and technology media. Once the preserve of the military, several factors have led to multicopter UAVs being fabricated and flown by hobbyist pilots:
\begin{description}
\item[The Maker Movement]{Affordable 3D printing have made it easier to prototype and produce cheap multicopter airframes.}
\item[Commodity Embedded Electronics]{A number of projects, beginning with the Arduino platform, have made well supported embedded computers available at low cost to the general public.}
\item[Open Source Software]{There are a number of popular open source autopilot projects that have significantly lowered the barrier to entry to novice users.}
\item[Smartphone IMUs]{As smartphones have grown in popularity, it has become possible to source low cost inertial measurement units (IMUs) which measurement of acceleration along several axes.}
\item[Ease of Control]{Multicopter UAVs offer the vertial take off and landing versatility of a traditional rotorcraft but with increased ease of control.}
\end{description}

These factors allow UAVs to be bought off the shelf for less than a thousand dollars, which is several orders of magnitude less than the cheapest traditional UAV. As with any disruptive technology, there is growing interest in the potential commercial applications. 

However, before these craft can be used reliably in a commercial setting, there are a number of shortfalls. This project aims to address two of these:
\begin{enumerate}
\item{Multicopter craft consume a lot of power. Battery life is limited to 30 minutes for the most efficient craft with large batteries.}
\item{Autonomous landing relies on GPS for localisation. This has limited accuracy.}
\end{enumerate}

\subsection{Automated Charging}

In a commercial context, UAVs become limited by their short battery life. If a user wishes to perform a mission that takes longer than the battery life of a UAV, they have to operate multiple UAVs and be response for battery handling (which typically involves removing a battery, plugging it into specialised equipment for at 30-60 minutes). This adds to the operational complexity and ultimately the cost for operating UAVs.

% TODO: Add references
One approach to mitigating the short battery life of a UAV is to have it land on a charging station. 

\subsubsection{Inductive Charging}
Several approaches have used inductive charging, where an oscillating magnetic field in the base of the charging station induces current in a coil on the UAV and charges the battery. While this approach allows for considerable inaccuracy in landing, it is approximately 20x slower than conductive charging and requires extra equipment on the UAV itself which impedes its aerodynamic efficiency.

\subsubsection{Conductive Charging}
A more traditional approach is to consider conductive charging. This is extremely efficient and offers very fast charge times. Additionally, it requires minimal modification to the airframe itself. However, contacts have to be positioned very accurately. As part of an ongoing Master's capstone project, we have designed a mechanically actuated charging station that uses an arm to bring a UAV into charging contacts (Figure \ref{fig:catia}). 

This approach requires accurate landing of the UAV onto a predefined area of the charging station. Some inaccuracy can be tolerated by the mechanical arm but in order to minimise the physical size of the station, it is necessary to land with some accuracy. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/catia.jpg}
    \caption{Our design for a UAV charging station.}
    \label{fig:catia}
\end{figure*}


\subsection{Accurate Landing}
The de facto method of state estimation for outdoor UAVs is through sensor fusion of IMU data, barometric pressure measurements, and GPS data. IMUs are used to provide estimates of the UAV's roll, pitch and yaw rates and barometric pressure estimates the UAVs altitude. GPS data is used to geographically localise the UAV. Our experiments showed that using the auto-landing functionality (in \textit{return to launch} mode) of our autopilot resulted in an accuracy of landing that tracks the known accuracy of GPS systems. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{images/marker.JPG}
    \caption{Testing landing accuracy using GPS}
    \label{fig:marker}
\end{figure*}

We experimentally gained 10 estimates for landing accuracy by drawing an X marker using yellow tape (Figure \ref{fig:marker}), arming the UAV at that location, flying it an arbitrary distance away and then invoking the \textit{return to launch} mode. We measured the straight line distance between the centre of the quadcopter and the centre of the marker. These results are shown in Table \ref{tbl:landing-data}, giving a mean accuracy of 195.33 centimetres and a standard deviation of 110.73 centimetres. 

\begin {table}
  \centerline{
  \begin{tabular}{ | c | c | }
  \hline
  Inches & Centimetres \\ 
  \hline
  22 & 55.88 \\ 
  63 & 160.02 \\ 
  39 & 99.06 \\ 
  84.5 & 214.63 \\ 
  25.5 & 64.77 \\ 
  76 & 193.04 \\ 
  158 & 401.32 \\ 
  130.5 & 331.47 \\ 
  76 & 193.04 \\ 
  94.5 & 240.03 \\ 
  \hline
  \end{tabular}}
   \caption{Distance of landing from home location using RTL}
   \label{tbl:landing-data}
\end {table}

Accurate landing is important for an automated charging station as described in the previous section. It is also important for many other applications of UAVs where it is necessary to land in and take off from constrained spaces. An example application is the use of UAVs as delivery vehicles to households. Many households may have limited clear space for the UAV to land. When using GPS, it is equally likely that the UAV will land on the household's roof as on their designed delivery point.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/drone.jpg}
    \caption{Our test quadcopter in action.}
    \label{fig:drone}
\end{figure*}

\subsection{Vision-Based Landing}
By augmenting our autopilot with an additional computer and webcam, we can use computer vision techniques to gain a highly accurate estimate of the UAVs pose with respect to a landing pad of a known design.
Our project was thus to:
\begin{enumerate}
\item{Implement a vision-based pose estimator.}
\item{Integrate our pose estimates into a controller that would land a UAV.}
\item{Demonstrate accurate autonomous landing using commodity electronics on a low cost quadrocopter UAV.}
\item{Make our software and implementation details openly available.}
\end{enumerate}

\section{Prior Work}

The automated landing problem has been investigated in the research literature
of the early 2000s. Here we briefly describe the approaches taken by some of
the most cited papers on this topic.

Sharp, Shakernia, and Sastry \cite{sharp_et_al_2001} designed an approach for
automatically landing an unmanned helicopter. Their landing target uses a
simple monochromatic design made up of several squares. Onboard the helicopter,
they use a pan-tilt-zoom camera and two embedded computers with Intel CPUs.
They discuss the details of their approach to pose estimation, but omit the
details of the helicopter controller. Using a real-time OS and optimized custom
code, they are able to get their vision system to operate at a rate of 30 Hz.

Our own approach in this project is modeled after that of Sharp et al. The main
differences are that (1) we use cheap off-the-shelf hardware, rather than
research-grade hardware; (2) our camera is stationary, it is not capable of
panning or tilting; and (3) we use off-the-shelf software components such as
ROS and OpenCV, rather than writing all of our code from scratch.

Saripalli, Montgomery, and Sukhatme \cite{saripalli_et_al_2002} designed
another approach for automatically landing an unmanned helicopter. They use a
monochromatic H-shaped landing target. Their onboard vision system detects this
landing target and outputs the helicopter's relative position with respect to
it. This is sent wirelessly to a behavior-based controller running on a ground
station, which then directs the helicopter to land on top of the target. They
are able to run their controller at 10 Hz this way. They are also using a
high-accuracy differential GPS system, and it is not clear how much their
differential GPS and vision systems contribute to a successful landing.

Garcia-Pardo, Sukhatme, and Montgomery \cite{garcia_pardo_et_al_2002} look at a
more general problem, where there is no pre-specified landing target, and their
helicopter has to search autonomously for a suitable clear area on which to
land.


\section{Approach}

\subsection{Architecture}
%% CONSTANTIN

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/architecture.png}
    \caption{Architecture of our automated landing system for UAVs}
    \label{fig:architecture}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/hardware.jpg}
    \caption{Our hardware stack installed on a quadcopter.}
    \label{fig:hardware}
\end{figure*}

% Put in weight of hardware stack and estimated power consumption?
% Estimated cost of system? Commodity hardware, etc.

% Hardware
% APM autopilot board
% BeagleBone black
% Logitech C920 Webcam (chosen for global shutter, to hopefully minimise motion blur)
% Powered USB hub
% EyePi wi-fi adaptor

% Software + Libraries Used
% ArduCopter autopilot software
% ROS
% Arm HF Linux
% roscopter library
% OpenCV

\subsection{Vision}
%% NAHUSH 
% Describe the vision approach we took

\subsubsection{Corner Detection}
%% NAHUSH

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/corners.png}
    \caption{Corner detection in action.}
    \label{fig:corners}
\end{figure*}

\subsubsection{Pose Estimation}
%% CONSTANTIN

Input: 24 point correspondences
Output: camera pose (x, y, z, roll, pitch, yaw)
Have 48 equations (2 for each point pair)
System of equations has 6 degrees of freedom
Solve it using an SVD trick


\subsection{Control}
%% SUNIl
In order to actually land our UAV using our pose estimates, it was necessary to implement a landing controller. Since this would need to run in real time (in order to adequately stabilise the UAV), we deferred real-time control to the \textit{loiter} mode provided by the ArduCopter autopilot. This takes care of real-time stabilisation for us and, additionally, attempts to keep the UAV in the same location using GPS. (They also offer a more basic stabilise mode which doesn't attempt to keep the craft to one location.)

Using the roscopter library, we were able to override the radio control inputs that would typically be received from the user. Using these inputs, we could manipulate the throttle, yaw, pitch and roll of the UAV.

\subsubsection{State Machine}
We designed a state machine with five states, beginning in a \textit{FLYING} mode and ending in \textit{POWER\_OFF}. Figure \ref{fig:statediagram} shows the transitions between the states. 

\paragraph{FLYING}
The \textit{FLYING} state represents when the UAV is not under our control, for instance, when the user is in manual control or has invoked another autopilot mode. Our controller listens to a specific channel of the radio input which represents three fixed values that are user selectable on the handheld transmitter. When the value of that channel changes to a predefined value, our controller takes control of the autopilot and transitions into \textit{SEEK\_HOME}.

\paragraph{SEEK\_HOME}
In \textit{SEEK\_HOME} mode, we use the built in \textit{return to launch} mode that ArduCopter implements - this navigates the UAV back to the launch location. Once above the launch location, the UAV should start receiving valid pose estimates.

\paragraph{LAND\_HIGH}
Our controller enters the \textit{LAND\_HIGH} mode when it starts receiving valid pose estimates. We use a simple proportional controller to calculate the appropriate control inputs based on the deviation from the centre of the landing station (in terms of x, y and yaw). These comprise the error terms in our controller.

Our controller disregards the z deviation and descends at a fixed rate. The z deviation is, however, used as the best estimate of altitude (our tests, described in section SECTION, show that the calculated z value is extremely accurate). When the UAV reaches a predefined altitude (where we know pose estimates will no longer be possible, due to field of view limitations), our controller enters into the \textit{LAND\_LOW} state.

\paragraph{LAND\_LOW}
In \textit{LAND\_LOW}, we use a dead reckoning approach to the ground since we are no longer able to receive valid pose estimates. Our controller attempts to keep the UAV flying straight and level and descends at a fixed rate. It uses data from the barometric pressure sensor to determine when the UAV has reached the ground and then enters into the \textit{POWER\_OFF} state.

\paragraph{POWER\_OFF}
\textit{POWER\_OFF} retains control of the UAV but sets throttle to a low value to prevent it from flying.

\begin{figure*}[h]
    \centering
    \includegraphics{images/statediagram.png}
    \caption{State diagram of our landing controller.}
    \label{fig:statediagram}
\end{figure*}

\subsection{Software Integration}
%% SUNIL
In order to integrate the three major components of software (roscopter, our pose estimator and our landing controller), we made extensive use of the publish and subscribe mechanism built into ROS. This allowed us to implement landing control in an event driven style - new control messages are generated upon receipt by the landing controller of new state information. This may include an updated pose estimate, an input from the user's radio control transmitter or new sensor data from the autopilot. Since the time taken to generate new control messages is considerably less than that to generate pose estimates, this allows us to generate control messages as frequently as we get new pose estimates. Figure \ref{fig:rosnodes} shows this setup.

\begin{figure*}[h]
    \centering
    \includegraphics{images/rosnodes.png}
    \caption{Setup of our ROS nodes.}
    \label{fig:rosnodes}
\end{figure*}

\section{Results}

\subsection{Pose Estimation Accuracy}
%% CONSTANTIN

% true height	z mean	z std	x std	y std	yaw std
% 88 cm	89.3 cm	0.05 cm	0.43 cm	0.39 cm	0.12 °
% 120 cm	121.1 cm	0.08 cm	1.16 cm	1.06 cm	0.12 °
% 170 cm	172.0 cm	0.18 cm	2.74 cm	2.17 cm	0.07 °
% 226 cm	229.0 cm	0.54 cm	6.51 cm	6.05 cm	0.34 °


\subsection{Performance}
%% CONSTANTIN

Camera is capable of 30 FPS
Just capturing frames: 11.2 FPS
Pose estimator running: 3.0 FPS
Pose estimator and roscopter running: 1.6 FPS

% Rate our system against the system they had in 2001 which ran at near real time FPS

\subsection{Control}

%% SUNIL 
% How did our controller do?

\subsubsection{Transitions}

% Field of view of camera and size of landing pad meant no accurate Z estimates
% below ~ 2m.
% Noisy sensor data = state transitions wer edifficult

\section{Challenges}
One of the significant impediments to our project was integrating working hardware. In this section, we describe the issues we had and how they might be mitigated in the future.

\subsection{Integration of Hardware}
%% CONSTANTIN
% A brief note on the hardware travails we had.

\subsection{Image Quality}
%% NAHUSH
% Mention the issue with motion blur up in the air and possible solutions (better camera with more fine grained control)

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/badimage.jpg}
    \caption{An example of a bad image.}
    \label{fig:badimage}
\end{figure*}

\subsection{Field of View}
%% NAHUSH

% Drone needs to be at 1 metre to see the landing pad.
% Practically, this means we need to be at about 2 metres to get
% pose estimates.

% We could use a concentric landing pattern to mitigate this, or makae the landing pad smaller.

% height	visible land area
% 1 m	1.37 m x 0.77 m
% 2 m	2.75 m x 1.54 m
% 4 m	5.50 m x 3.07 m
% 8 m	11.0 m x 6.14 m
% 16 m	22.0 m x 12.3 m

\begin{figure*}[h]
    \centering
    \includegraphics{images/fov.png}
    \caption{Field of view of our webcam.}
    \label{fig:fov}
\end{figure*}

\subsection{Weather}
%% SUNIL
The weather conditions became more limiting than we had initially imagined - in particular due to the short term nature of this project. Two factors worked against us and made testing difficult.

\subsubsection{Temperature}
As mentioned in the Motivation section, multicopter UAVs have a limited battery life. The underlying battery technology is affected by the cold weather which meant that flight times, even with the largest 5500 mAh batteries on hand, were limited to 5 or 6 minutes before needing replacement. This minimised the amount of testing we could do per charging cycle.

Besides carrying a larger number of batteries out to the field, there is not much we can do to mitigate this.

\subsubsection{Wind}
Our basic proportional controller struggled when there were large amounts of wind present. This is because our controller doesn't operate quickly enough to react to changes in the UAVs state, relying upon the \textit{loiter} mode of the autopilot to keep the UAV in the same place. Since the \textit{loiter} mode relies upon GPS to give us an estimate of the UAVs location, when the location deviates by a large amount, it tries to bring it back to the GPS location - which relies upon the relative inaccuracy of GPS.

This could be mitigated through better controller design. In particular: using a full Proportional-Integral-Derivative (PID) controller and by increasing the rate at which control messages are generated.

\paragraph{PID Controller}
Using a full PID controller will allow us to develop a more aggressive controller that is able to reduce the error (i.e. deviation from the centre of the landing pad) more quickly and in a way that is proportional to external noise (i.e. wind) - and not just the amount of noise.

\paragraph{Control Rate}
At the moment we gain pose-estimates at a slow rate (and provide control updates at the same rate) of approximately 1.6Hz. The GPS sensor included in the autopilot operates at 5Hz and it typically provides control updates at 10Hz. 10Hz is enough to provide real-time control of the quadrocopter UAV that we use and so increasing the performance of the pose estimator up to between 5 and 10Hz may be enough to better offset windy flying conditions.

\section{Conclusion \& Future Directions}

Use higher quality optics.
Explore landing pad design.
Allow landing in darker scenarios.
Use more advanced control loops (PID instead of just P).
Rewrite roscopter in C++.
Integrate sonar sensor for more accurate altitude estimates.

\printbibliography

\end{document}
